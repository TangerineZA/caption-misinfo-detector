{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# from transformers import pipeline\n",
    "from helpers import Video_loader, Image_loader, Captioner, Audio_transcriber, Fusion_helper\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOS_DIRECTORY = \"videos\"\n",
    "FRAME_FREQUENCY = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init video loader\n",
      "Make frames\n",
      "Storing in videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_images\n",
      "Start image loading\n",
      "Init image loader\n",
      "Init captioner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBlipForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBlipForConditionalGeneration were initialized from the model checkpoint at Salesforce/blip-image-captioning-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBlipForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start image get captions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\research-env\\lib\\site-packages\\transformers\\generation\\tf_utils.py:854: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TFBlipTextModel.get_extended_attention_mask at 0x0000019E27D9CB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TFBlipTextModel.get_extended_attention_mask at 0x0000019E27D9CB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "------------------------------------------------------------------------------------------\n",
      " END OF HELPERS \n",
      "\n",
      "{'videos\\\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_images': [[{'generated_text': \"the logo for los's employee training\"}], [{'generated_text': 'a cartoon depicting a man cleaning a room'}], [{'generated_text': 'a man in a yellow shirt and tie is pointing at a computer screen'}], [{'generated_text': 'a man and woman are in a room with a tv screen'}], [{'generated_text': 'a man with glasses and a thought bubble above him'}], [{'generated_text': 'a man in a tie and shirt standing in a restaurant'}], [{'generated_text': 'a man with a mustache and a blue sky in the background'}], [{'generated_text': 'a man in a tie standing in front of a sign'}], [{'generated_text': 'a man with a beard and a blue sky in the background'}], [{'generated_text': 'a man in a tie and glasses is making a funny face'}], [{'generated_text': 'a man in a tie and shirt standing in a room'}]]}\n"
     ]
    }
   ],
   "source": [
    "# video captions section\n",
    "video_loader = Video_loader(VIDEOS_DIRECTORY, FRAME_FREQUENCY)\n",
    "\n",
    "captions_dict = video_loader.process()\n",
    "# need to tune this up but for now it's whatever - we have frame visual captions now\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------------\\n END OF HELPERS \\n\")\n",
    "print(captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing video folder: videos\n",
      "Filenames: \n",
      "[<DirEntry 'Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4'>]\n",
      "Make folders and save audio\n",
      "Storing in videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\n",
      "Transcribing video <DirEntry 'Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4'>\n",
      "Audio split!\n",
      "videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\\chunk1.wav : hello and welcome to the Los Pollos Hermanos family my name is Gustavo but you can call me I am thrilled that you'll be joining our team each and every day we serve our customers exceptional we take pride everything that we do and after this 10-week online seminar I am confident that you would write it I like to think I see things and people to begin I'd like to talk about the cornerstone of the Los Pollos Hermanos brand communication as an employee have lost boy from are you set the tone for the entire dining experience be mindful of what your word and behaviour communicate to Oregon\n",
      "videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\\chunk2.wav : always be aware of Your Mind remember to stand up straight your customers and your back will thank you for it put effort into your appearance all employees are required to dress appropriately keep your uniform clean and press if you want respect you must look respected\n",
      "videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\\chunk3.wav : speak in complete sentences we never use one word greetings like hey yeah always make eye contact and finally what do you do with the class tomorrow night remain composed\n",
      "videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\\chunk4.wav : inside you can be thinking about your home or your friends or your side business but no one should ever know because at Los Pollos Romano someone\n",
      "videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\\chunk5.wav : he's always watching\n",
      "videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\\chunk6.wav : don't forget to smile that's all for today see you next\n",
      "videos\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_audio\\chunk7.wav : will be discussing\n",
      "Error: \n",
      "Returning whole text! hello and welcome to the Los Pollos Hermanos family my name is Gustavo but you can call me I am thrilled that you'll be joining our team each and every day we serve our customers exceptional we take pride everything that we do and after this 10-week online seminar I am confident that you would write it I like to think I see things and people to begin I'd like to talk about the cornerstone of the Los Pollos Hermanos brand communication as an employee have lost boy from are you set the tone for the entire dining experience be mindful of what your word and behaviour communicate to Oregon always be aware of Your Mind remember to stand up straight your customers and your back will thank you for it put effort into your appearance all employees are required to dress appropriately keep your uniform clean and press if you want respect you must look respected speak in complete sentences we never use one word greetings like hey yeah always make eye contact and finally what do you do with the class tomorrow night remain composed inside you can be thinking about your home or your friends or your side business but no one should ever know because at Los Pollos Romano someone he's always watching don't forget to smile that's all for today see you next will be discussing\n"
     ]
    }
   ],
   "source": [
    "# audio captions section\n",
    "audio_transcriber = Audio_transcriber(VIDEOS_DIRECTORY)\n",
    "audio_captions_dict = audio_transcriber.transcribe_video_folder(VIDEOS_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---FRAME CAPTIONS---\n",
      "{'videos\\\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_images': [[{'generated_text': \"the logo for los's employee training\"}], [{'generated_text': 'a cartoon depicting a man cleaning a room'}], [{'generated_text': 'a man in a yellow shirt and tie is pointing at a computer screen'}], [{'generated_text': 'a man and woman are in a room with a tv screen'}], [{'generated_text': 'a man with glasses and a thought bubble above him'}], [{'generated_text': 'a man in a tie and shirt standing in a restaurant'}], [{'generated_text': 'a man with a mustache and a blue sky in the background'}], [{'generated_text': 'a man in a tie standing in front of a sign'}], [{'generated_text': 'a man with a beard and a blue sky in the background'}], [{'generated_text': 'a man in a tie and glasses is making a funny face'}], [{'generated_text': 'a man in a tie and shirt standing in a room'}]]}\n",
      "\n",
      "\n",
      "---AUDIO CAPTIONS---\n",
      "{<DirEntry 'Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4'>: \" hello and welcome to the Los Pollos Hermanos family my name is Gustavo but you can call me I am thrilled that you'll be joining our team each and every day we serve our customers exceptional we take pride everything that we do and after this 10-week online seminar I am confident that you would write it I like to think I see things and people to begin I'd like to talk about the cornerstone of the Los Pollos Hermanos brand communication as an employee have lost boy from are you set the tone for the entire dining experience be mindful of what your word and behaviour communicate to Oregon always be aware of Your Mind remember to stand up straight your customers and your back will thank you for it put effort into your appearance all employees are required to dress appropriately keep your uniform clean and press if you want respect you must look respected speak in complete sentences we never use one word greetings like hey yeah always make eye contact and finally what do you do with the class tomorrow night remain composed inside you can be thinking about your home or your friends or your side business but no one should ever know because at Los Pollos Romano someone he's always watching don't forget to smile that's all for today see you next will be discussing\"}\n"
     ]
    }
   ],
   "source": [
    "# explore dicts\n",
    "print(\"---FRAME CAPTIONS---\")\n",
    "print(captions_dict)\n",
    "print(\"\\n\\n---AUDIO CAPTIONS---\")\n",
    "print(audio_captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'videos\\\\Los Pollos Hermanos Employee Training_ Communication _ Better Call Saul.mp4_images': \" the logo for los's employee training a cartoon depicting a man cleaning a room a man in a yellow shirt and tie is pointing at a computer screen a man and woman are in a room with a tv screen a man with glasses and a thought bubble above him a man in a tie and shirt standing in a restaurant a man with a mustache and a blue sky in the background a man in a tie standing in front of a sign a man with a beard and a blue sky in the background a man in a tie and glasses is making a funny face a man in a tie and shirt standing in a room\"}\n"
     ]
    }
   ],
   "source": [
    "# generate new frame caption dict with cleaned data\n",
    "video_captions_dict = {}\n",
    "for video_name in captions_dict.keys():\n",
    "    combined_text = \"\"\n",
    "    for top_level in captions_dict[video_name]:\n",
    "        inside_dict = top_level[0]\n",
    "        combined_text = combined_text + ' ' + inside_dict[\"generated_text\"]\n",
    "    video_captions_dict[video_name] = combined_text\n",
    "\n",
    "print(video_captions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting full vocabulary\n",
      "Building embedding layer from wordlist\n",
      "Getting embeddings from wordlist\n",
      "Building embedding layer\n",
      "Building vectorise layer\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 4, max vocab size is 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m embedding_layer \u001b[39m=\u001b[39m fusion_helper\u001b[39m.\u001b[39mbuild_embedding_layer_from_wordlist(final_vocab)\n\u001b[0;32m      9\u001b[0m \u001b[39m# TODO examine whether vectorisation and embedding layers are crosscompatible\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m vectorise_layer \u001b[39m=\u001b[39m fusion_helper\u001b[39m.\u001b[39;49mbuild_vectorise_layer_from_wordlist(final_vocab)\n\u001b[0;32m     11\u001b[0m \u001b[39m# yay - embedding layer made! going to also need a text vectorisation layer though that turns strings into vocab indices\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\Desktop\\tiktec\\mini-model\\helpers.py:288\u001b[0m, in \u001b[0;36mFusion_helper.build_vectorise_layer_from_wordlist\u001b[1;34m(self, wordlist)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_vectorise_layer_from_wordlist\u001b[39m(\u001b[39mself\u001b[39m, wordlist : \u001b[39mlist\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mTextVectorization:\n\u001b[0;32m    287\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBuilding vectorise layer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 288\u001b[0m     layer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mTextVectorization(\n\u001b[0;32m    289\u001b[0m         max_tokens \u001b[39m=\u001b[39;49m (\u001b[39mlen\u001b[39;49m(wordlist) \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m),\n\u001b[0;32m    290\u001b[0m         output_mode \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mint\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    291\u001b[0m         vocabulary \u001b[39m=\u001b[39;49m wordlist\n\u001b[0;32m    292\u001b[0m     )\n\u001b[0;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\research-env\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:389\u001b[0m, in \u001b[0;36mTextVectorization.__init__\u001b[1;34m(self, max_tokens, standardize, split, ngrams, output_mode, output_sequence_length, pad_to_max_tokens, vocabulary, idf_weights, sparse, ragged, encoding, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    385\u001b[0m base_preprocessing_layer\u001b[39m.\u001b[39mkeras_kpl_gauge\u001b[39m.\u001b[39mget_cell(\n\u001b[0;32m    386\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTextVectorization\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    387\u001b[0m )\u001b[39m.\u001b[39mset(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lookup_layer \u001b[39m=\u001b[39m string_lookup\u001b[39m.\u001b[39;49mStringLookup(\n\u001b[0;32m    390\u001b[0m     max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[0;32m    391\u001b[0m     vocabulary\u001b[39m=\u001b[39;49mvocabulary,\n\u001b[0;32m    392\u001b[0m     idf_weights\u001b[39m=\u001b[39;49midf_weights,\n\u001b[0;32m    393\u001b[0m     pad_to_max_tokens\u001b[39m=\u001b[39;49mpad_to_max_tokens,\n\u001b[0;32m    394\u001b[0m     mask_token\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    395\u001b[0m     output_mode\u001b[39m=\u001b[39;49moutput_mode \u001b[39mif\u001b[39;49;00m output_mode \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m INT,\n\u001b[0;32m    396\u001b[0m     sparse\u001b[39m=\u001b[39;49msparse,\n\u001b[0;32m    397\u001b[0m     has_input_vocabulary\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_has_input_vocabulary,\n\u001b[0;32m    398\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    399\u001b[0m     vocabulary_size\u001b[39m=\u001b[39;49mvocabulary_size,\n\u001b[0;32m    400\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\research-env\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\string_lookup.py:334\u001b[0m, in \u001b[0;36mStringLookup.__init__\u001b[1;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, idf_weights, encoding, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    332\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding \u001b[39m=\u001b[39m encoding\n\u001b[1;32m--> 334\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    335\u001b[0m     max_tokens\u001b[39m=\u001b[39mmax_tokens,\n\u001b[0;32m    336\u001b[0m     num_oov_indices\u001b[39m=\u001b[39mnum_oov_indices,\n\u001b[0;32m    337\u001b[0m     mask_token\u001b[39m=\u001b[39mmask_token,\n\u001b[0;32m    338\u001b[0m     oov_token\u001b[39m=\u001b[39moov_token,\n\u001b[0;32m    339\u001b[0m     vocabulary\u001b[39m=\u001b[39mvocabulary,\n\u001b[0;32m    340\u001b[0m     vocabulary_dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mstring,\n\u001b[0;32m    341\u001b[0m     idf_weights\u001b[39m=\u001b[39midf_weights,\n\u001b[0;32m    342\u001b[0m     invert\u001b[39m=\u001b[39minvert,\n\u001b[0;32m    343\u001b[0m     output_mode\u001b[39m=\u001b[39moutput_mode,\n\u001b[0;32m    344\u001b[0m     sparse\u001b[39m=\u001b[39msparse,\n\u001b[0;32m    345\u001b[0m     pad_to_max_tokens\u001b[39m=\u001b[39mpad_to_max_tokens,\n\u001b[0;32m    346\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m base_preprocessing_layer\u001b[39m.\u001b[39mkeras_kpl_gauge\u001b[39m.\u001b[39mget_cell(\u001b[39m\"\u001b[39m\u001b[39mStringLookup\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mset(\n\u001b[0;32m    349\u001b[0m     \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    350\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\research-env\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:323\u001b[0m, in \u001b[0;36mIndexLookup.__init__\u001b[1;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary_dtype, vocabulary, idf_weights, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf_weights_const \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf_weights\u001b[39m.\u001b[39mvalue()\n\u001b[0;32m    322\u001b[0m \u001b[39mif\u001b[39;00m vocabulary \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 323\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_vocabulary(vocabulary, idf_weights)\n\u001b[0;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[39m# When restoring from a keras SavedModel, the loading code will\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[39m# expect to find and restore a lookup_table attribute on the layer.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[39m# This table needs to be uninitialized as a StaticHashTable cannot\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[39m# be initialized twice.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_uninitialized_lookup_table()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\research-env\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:576\u001b[0m, in \u001b[0;36mIndexLookup.set_vocabulary\u001b[1;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[0;32m    574\u001b[0m new_vocab_size \u001b[39m=\u001b[39m token_start \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(tokens)\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (new_vocab_size \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens):\n\u001b[1;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to set a vocabulary larger than the maximum vocab \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msize. Passed vocab size is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, max vocab size is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    579\u001b[0m             new_vocab_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens\n\u001b[0;32m    580\u001b[0m         )\n\u001b[0;32m    581\u001b[0m     )\n\u001b[0;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lookup_table_from_tokens(tokens)\n\u001b[0;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_record_vocabulary_size()\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 4, max vocab size is 3."
     ]
    }
   ],
   "source": [
    "# fusion section\n",
    "# steps to achieve:\n",
    "# - combine string per video, do word embeddings with GloVe - theoretically, helper function here is complete!\n",
    "# - build fusion layer - must see how to integrate the embeddings now\n",
    "fusion_helper = Fusion_helper()\n",
    "all_dicts = [video_captions_dict, audio_captions_dict]\n",
    "final_vocab = fusion_helper.get_full_vocabulary(all_dicts)\n",
    "embedding_layer = fusion_helper.build_embedding_layer_from_wordlist(final_vocab)\n",
    "# TODO examine whether vectorisation and embedding layers are crosscompatible\n",
    "vectorise_layer = fusion_helper.build_vectorise_layer_from_wordlist(final_vocab)\n",
    "# yay - embedding layer made! going to also need a text vectorisation layer though that turns strings into vocab indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model itself:\n",
    "model = tf.keras.Sequential([\n",
    "    vectorise_layer,\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compilation\n",
    "# steps to achieve:\n",
    "# - implement transformer model to analyse all text\n",
    "# - decide on loss - binary cross-entropy most likely\n",
    "# - decide on optimiser\n",
    "# - compile model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "# steps to achieve:\n",
    "# - download all data\n",
    "# - run preprocessing on all of the videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
